{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed700c0f-0a84-4e64-9842-8d1556265948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "041b7b28-ba6f-4b4b-96ad-3194fe1d4d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy example: you left the house and realise you don't have your keys. where are they?\n",
    "# this is the most naive an inefficient text adventure on purpose.\n",
    "\n",
    "#world = [('description', reward, [nested_world])]\n",
    "class find_the_keys():\n",
    "    \"A little test world to get going\"\n",
    "    def __init__(self):\n",
    "        self.world = (\n",
    "            'look around', 0, [\n",
    "                ('go left', 0, [\n",
    "                    ('climb tree', 0, []),\n",
    "                    ('search floor', 0, [\n",
    "                        ('lift stone', 0,[]),\n",
    "                        ('lift leaf', 0,[])\n",
    "                    ])\n",
    "                ]),\n",
    "                ('go straight', 0, [\n",
    "                    ('enter house', 0, [\n",
    "                        ('check cupboard', 1, []),\n",
    "                        ('check wardrobe', 0, [])\n",
    "                    ])\n",
    "                ]),\n",
    "                ('go right', 0, [\n",
    "                    ('check bike', 0, []), # bike is locked\n",
    "                    ('check mailbox', 0, [\n",
    "                        ('open first letter', 0, []),\n",
    "                        ('open second letter', 0, []),\n",
    "                        ('open third letter', 0, []),\n",
    "                    ])\n",
    "                ])\n",
    "            ]\n",
    "        )\n",
    "        self.reset()\n",
    "        \n",
    "        \n",
    "    def _get_state(self):\n",
    "        state = self.world\n",
    "        description = state[0]\n",
    "        reward = state[1]\n",
    "        actions = state[2]\n",
    "        for a in self.previous_actions:\n",
    "            description, reward, actions = actions[a]\n",
    "        return description, reward, actions\n",
    "    \n",
    "    def _set_state(self, previous_actions):\n",
    "        self.previous_actions = previous_actions\n",
    "        self.description, self.reward, self.actions = self._get_state()\n",
    "        \n",
    "    def reset(self):\n",
    "        self._set_state([])\n",
    "    \n",
    "    def get_description(self):\n",
    "        return self.description\n",
    "    \n",
    "    def get_reward(self):\n",
    "        return self.reward\n",
    "    \n",
    "    def get_actions(self):\n",
    "        return [(a[0], a[1]) for a in self.actions] + [('go back', 0)]\n",
    "    \n",
    "    def do(self, action):\n",
    "        if action == 'go back':\n",
    "            self._set_state(self.previous_actions[:-1])\n",
    "            return \"done\"\n",
    "        else:\n",
    "            for i, a in enumerate(game.get_actions()):\n",
    "                if a[0] == action:\n",
    "                    self._set_state(self.previous_actions + [i])\n",
    "                    return \"done\"\n",
    "        return \"impossible\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e55f7b4-48cd-4cf7-85c9-e16614f741cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = find_the_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3796b36-6426-4102-9b1c-7dfd90b4f57f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'look around'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.get_description()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d6a2643-d12b-4986-b0dd-e344338b82fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.get_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e544ea7-b84d-4c77-93b0-b3fe39e71456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('go left', 0), ('go straight', 0), ('go right', 0), ('go back', 0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.get_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d88e9212-df51-4adf-a4b6-6fd5dbe9682a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'done'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.do('go left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28ef97e1-46e2-44ab-aa40-fbf8e936e444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('climb tree', 0), ('search floor', 0), ('go back', 0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.get_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b854335d-a31a-4c3c-ba15-ffea8f9c9dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'done'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.do('go back')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e73f656b-96f4-4e74-8477-8d324bf6d3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('go left', 0), ('go straight', 0), ('go right', 0), ('go back', 0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.get_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4e6d067-91e4-46fa-bb82-22ce212f936b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# how to solve\n",
    "game.do('go straight')\n",
    "game.do('enter house')\n",
    "game.do('check cupboard')\n",
    "print(game.reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bb8323c-05e2-49f5-ab91-90b83a56518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random exploration\n",
    "game = find_the_keys()\n",
    "steps = 9000\n",
    "rewards = []\n",
    "rng = np.random.default_rng()\n",
    "for i in range(steps):\n",
    "    description = game.get_description()\n",
    "    actions = game.get_actions()\n",
    "    rewards.append(game.get_reward())\n",
    "    #print(description, reward)\n",
    "    if rewards[-1]:\n",
    "        # this only makes sense because of the specific game here\n",
    "        #print(\"You won! Reseting game.\")\n",
    "        game._set_state([])\n",
    "    else:\n",
    "        j = rng.integers(0, len(actions))\n",
    "        #print(\"do\", actions[j][0])\n",
    "        game.do(actions[j][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fddb3e10-6753-4002-b6c7-136203ea2493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(rewards)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e87234bb-951d-4588-b450-e5b51554d856",
   "metadata": {},
   "source": [
    "# q-learning\n",
    "# again the most naive way I could think of, see https://en.wikipedia.org/wiki/Q-learning\n",
    "game = find_the_keys()\n",
    "rewards = []\n",
    "q = {}\n",
    "q_init = 1\n",
    "learning_rate = .9 # \"alpha\"\n",
    "discount_factor = .1 # \"gamma\"\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "state = game.get_description()\n",
    "actions = game.get_actions()\n",
    "\n",
    "p_explore = 0.01\n",
    "\n",
    "steps = 9000\n",
    "verbose=False\n",
    "\n",
    "def get_q_s(q, state, actions, q_init):\n",
    "    q_s = q.get(state, {})\n",
    "    q_s = {a[0]: q_init for a in actions} | q_s # fill q for missing actions\n",
    "    return q_s\n",
    "\n",
    "for i in range(steps):\n",
    "    if verbose: print()\n",
    "    if verbose: print(\"step\", i, \"state\", state)\n",
    "    q_s = get_q_s(q, state, actions, q_init)\n",
    "    q[state] = q_s\n",
    "    if verbose: print(\"q\", q)\n",
    "    \n",
    "    if p_explore > rng.uniform():\n",
    "        if verbose: print(\"explore\")\n",
    "        j = rng.integers(0, len(actions))\n",
    "        action = actions[j][0]\n",
    "    else:\n",
    "        if verbose: print(\"exploit\")\n",
    "        if verbose: print(q_s)\n",
    "        action = max(q_s, key=q_s.get)\n",
    "        \n",
    "    if verbose: print(\"do\", action)\n",
    "    game.do(action)\n",
    "\n",
    "    new_state = game.get_description()\n",
    "    new_reward = game.get_reward()\n",
    "    new_actions = game.get_actions()\n",
    "    max_new_q = max(get_q_s(q, new_state, new_actions, q_init).values())\n",
    "    \n",
    "    if verbose: print(\"new reward\", new_reward, \"max new q\", max_new_q)\n",
    "\n",
    "    q_sa = q_s.get(action, q_init)\n",
    "    if verbose: print(\"q_sa\", q_sa)\n",
    "    q_s[action] = (1 - learning_rate) * q_sa + learning_rate * (new_reward + discount_factor * max_new_q)\n",
    "    if verbose: print(f\"new q\", q)\n",
    "    \n",
    "    rewards.append(new_reward)\n",
    "    \n",
    "    if new_reward:\n",
    "        game.reset()\n",
    "        new_state = game.get_description()\n",
    "        new_actions = game.get_actions()\n",
    "\n",
    "    state = new_state\n",
    "    actions = new_actions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "979d6652-8645-4c32-ac26-bc282b330014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q-learning\n",
    "# again the most naive way I could think of, see https://en.wikipedia.org/wiki/Q-learning\n",
    "game = find_the_keys()\n",
    "rewards = []\n",
    "\n",
    "class q_learner():\n",
    "    \"\"\"Naive q-learning agent\n",
    "    \n",
    "    Implementing reward table \"q[state,action]\" as a key-value map\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        q_init:float = 1, # initial q > 0 incentivises exploration\n",
    "        learning_rate:float = .9, # \"alpha\"\n",
    "        discount_factor:float = .1, # \"gamma\"\n",
    "        p_explore:float = 0.01,\n",
    "        rng:np.random.Generator = np.random.default_rng() # allows to control randomness\n",
    "    ):\n",
    "        self.q = {} # the reward \"table\"\n",
    "        self.q_init = q_init\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.p_explore = p_explore\n",
    "        self.rng = rng\n",
    "        self.verbose = False\n",
    "    \n",
    "    def get_q_s(self,state:str, actions:list):\n",
    "        q_s = self.q.get(state, {})\n",
    "        q_s = {a[0]: self.q_init for a in actions} | q_s # fill q for missing actions\n",
    "        return q_s\n",
    "    \n",
    "    def choose_action(self, q_s, state, actions):\n",
    "        \"explore or choose action maximising q\"\n",
    "        #q_s = self.get_q_s(state, actions)\n",
    "        if self.p_explore > self.rng.uniform():\n",
    "            if self.verbose: print(\"explore\")\n",
    "            j = self.rng.integers(0, len(actions))\n",
    "            action = actions[j][0]\n",
    "        else:\n",
    "            if self.verbose: print(\"exploit\")\n",
    "            if self.verbose: print(q_s)\n",
    "            action = max(q_s, key=q_s.get)\n",
    "        return action\n",
    "    \n",
    "    def learn(self, q_s, action, new_state, new_actions):\n",
    "        max_new_q = max(agent.get_q_s(new_state, new_actions).values())\n",
    "        q_s = self.get_q_s(state, actions)\n",
    "        q_sa = q_s.get(action, self.q_init)\n",
    "        if verbose: print(\"q_sa\", q_sa)\n",
    "        q_s[action] = (\n",
    "            (1 - self.learning_rate) * q_sa\n",
    "            + self.learning_rate * (new_reward + self.discount_factor * max_new_q)\n",
    "        )\n",
    "        self.q[state] = q_s\n",
    "        if verbose: print(f\"new q\", self.q)\n",
    "\n",
    "\n",
    "agent = q_learner()\n",
    "state = game.get_description()\n",
    "actions = game.get_actions()\n",
    "steps = 9000\n",
    "verbose=False\n",
    "agent.verbose=verbose\n",
    "\n",
    "for i in range(steps):\n",
    "    if verbose: print()\n",
    "    if verbose: print(\"step\", i, \"state\", state)\n",
    "    \n",
    "    q_s = agent.get_q_s(state, actions)\n",
    "    action = agent.choose_action(q_s, state, actions)\n",
    "    \n",
    "    if verbose: print(\"do\", action)\n",
    "    game.do(action)\n",
    "\n",
    "    new_state = game.get_description()\n",
    "    new_reward = game.get_reward()\n",
    "    new_actions = game.get_actions()\n",
    "    \n",
    "    if verbose: print(\"new resward\", new_reward, \"max new q\", max_new_q)\n",
    "\n",
    "    agent.learn(q_s, action, new_state, new_actions)\n",
    "    if verbose: print(\"q\", agent.q)\n",
    "    \n",
    "    rewards.append(new_reward)\n",
    "    \n",
    "    if new_reward:\n",
    "        game.reset()\n",
    "        new_state = game.get_description()\n",
    "        new_actions = game.get_actions()\n",
    "\n",
    "    state = new_state\n",
    "    actions = new_actions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c49cbb17-2fe0-4533-95c0-9340a8e0210b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('check cupboard', 1), ('check wardrobe', 0), ('go back', 0)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a32d1071-53e3-4ee4-ade8-6a1fedf75033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2932"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "156ab62c-ef2b-4e23-8255-5b951ed62196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'look around': {'go left': 0.00011000063279800003,\n",
       "  'go straight': 0.011000000000000003,\n",
       "  'go right': 0.00011000000076868903,\n",
       "  'go back': 0.0011057590000000002},\n",
       " 'go left': {'climb tree': 0.00024760989999999996,\n",
       "  'search floor': 0.0004247568999999999,\n",
       "  'go back': 0.0011000000575900003},\n",
       " 'climb tree': {'go back': 0.0002345041},\n",
       " 'search floor': {'lift stone': 0.00024760989999999996,\n",
       "  'lift leaf': 0.0004444398999999999,\n",
       "  'go back': 0.000281152},\n",
       " 'lift stone': {'go back': 0.00024760989999999996},\n",
       " 'lift leaf': {'go back': 0.0003205098999999999},\n",
       " 'go straight': {'enter house': 0.11000000000000001,\n",
       "  'go back': 0.0011000000000116789},\n",
       " 'enter house': {'check cupboard': 1.1,\n",
       "  'check wardrobe': 0.011000659600000004,\n",
       "  'go back': 0.011000098900000003},\n",
       " 'go right': {'check bike': 0.00024760989999999996,\n",
       "  'check mailbox': 0.00024760989999999996,\n",
       "  'go back': 0.0011000000000575903},\n",
       " 'check bike': {'go back': 0.00024760989999999996},\n",
       " 'check mailbox': {'open first letter': 0.00024760989999999996,\n",
       "  'open second letter': 0.00024760989999999996,\n",
       "  'open third letter': 0.00024760989999999996,\n",
       "  'go back': 0.0002345041},\n",
       " 'open first letter': {'go back': 0.00024760989999999996},\n",
       " 'open second letter': {'go back': 0.00024760989999999996},\n",
       " 'open third letter': {'go back': 0.00024760989999999996},\n",
       " 'check wardrobe': {'go back': 0.11000008900000002}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f40f4ed-4353-4171-a9e3-1c703917a333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
